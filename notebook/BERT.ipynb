{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LJHZPhB7GePJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "P9zcCG0NGMNi"
      },
      "outputs": [],
      "source": [
        "text='Science is the systematic pursuit of knowledge through observation, experimentation, and logical reasoning. It helps us understand the natural world, from the smallest particles to the vastness of the universe. Through scientific inquiry, humans uncover patterns, develop theories, and create technologies that improve daily life. Science encourages curiosity, critical thinking, and evidence-based decision-making. It evolves continuously as new discoveries challenge old ideas and expand our understanding. Whether exploring biological systems, studying energy, or investigating cosmic phenomena, science provides a reliable method for explaining how things work. Its progress shapes society and guides future innovation for the benefit of all.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBKv9ZXXXK5r",
        "outputId": "02d65371-bf76-466a-ce97-718841bdec9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Science',\n",
              " 'is',\n",
              " 'the',\n",
              " 'systematic',\n",
              " 'pursuit',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'through',\n",
              " 'observation',\n",
              " ',',\n",
              " 'experimentation',\n",
              " ',',\n",
              " 'and',\n",
              " 'logical',\n",
              " 'reasoning',\n",
              " '.',\n",
              " 'It',\n",
              " 'helps',\n",
              " 'us',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'natural',\n",
              " 'world',\n",
              " ',',\n",
              " 'from',\n",
              " 'the',\n",
              " 'smallest',\n",
              " 'particles',\n",
              " 'to',\n",
              " 'the',\n",
              " 'vastness',\n",
              " 'of',\n",
              " 'the',\n",
              " 'universe',\n",
              " '.',\n",
              " 'Through',\n",
              " 'scientific',\n",
              " 'inquiry',\n",
              " ',',\n",
              " 'humans',\n",
              " 'uncover',\n",
              " 'patterns',\n",
              " ',',\n",
              " 'develop',\n",
              " 'theories',\n",
              " ',',\n",
              " 'and',\n",
              " 'create',\n",
              " 'technologies',\n",
              " 'that',\n",
              " 'improve',\n",
              " 'daily',\n",
              " 'life',\n",
              " '.',\n",
              " 'Science',\n",
              " 'encourages',\n",
              " 'curiosity',\n",
              " ',',\n",
              " 'critical',\n",
              " 'thinking',\n",
              " ',',\n",
              " 'and',\n",
              " 'evidence-based',\n",
              " 'decision-making',\n",
              " '.',\n",
              " 'It',\n",
              " 'evolves',\n",
              " 'continuously',\n",
              " 'as',\n",
              " 'new',\n",
              " 'discoveries',\n",
              " 'challenge',\n",
              " 'old',\n",
              " 'ideas',\n",
              " 'and',\n",
              " 'expand',\n",
              " 'our',\n",
              " 'understanding',\n",
              " '.',\n",
              " 'Whether',\n",
              " 'exploring',\n",
              " 'biological',\n",
              " 'systems',\n",
              " ',',\n",
              " 'studying',\n",
              " 'energy',\n",
              " ',',\n",
              " 'or',\n",
              " 'investigating',\n",
              " 'cosmic',\n",
              " 'phenomena',\n",
              " ',',\n",
              " 'science',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'reliable',\n",
              " 'method',\n",
              " 'for',\n",
              " 'explaining',\n",
              " 'how',\n",
              " 'things',\n",
              " 'work',\n",
              " '.',\n",
              " 'Its',\n",
              " 'progress',\n",
              " 'shapes',\n",
              " 'society',\n",
              " 'and',\n",
              " 'guides',\n",
              " 'future',\n",
              " 'innovation',\n",
              " 'for',\n",
              " 'the',\n",
              " 'benefit',\n",
              " 'of',\n",
              " 'all',\n",
              " '.']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = [token for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip() != '']\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BQtXZtqXwlL",
        "outputId": "a7622856-696f-4b85-d74f-c174f31244f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'[CLS]': 0,\n",
              " '[SEP]': 1,\n",
              " '[MASK]': 2,\n",
              " '[PAD]': 3,\n",
              " ',': 4,\n",
              " '.': 5,\n",
              " 'It': 6,\n",
              " 'Its': 7,\n",
              " 'Science': 8,\n",
              " 'Through': 9,\n",
              " 'Whether': 10,\n",
              " 'a': 11,\n",
              " 'all': 12,\n",
              " 'and': 13,\n",
              " 'as': 14,\n",
              " 'benefit': 15,\n",
              " 'biological': 16,\n",
              " 'challenge': 17,\n",
              " 'continuously': 18,\n",
              " 'cosmic': 19,\n",
              " 'create': 20,\n",
              " 'critical': 21,\n",
              " 'curiosity': 22,\n",
              " 'daily': 23,\n",
              " 'decision-making': 24,\n",
              " 'develop': 25,\n",
              " 'discoveries': 26,\n",
              " 'encourages': 27,\n",
              " 'energy': 28,\n",
              " 'evidence-based': 29,\n",
              " 'evolves': 30,\n",
              " 'expand': 31,\n",
              " 'experimentation': 32,\n",
              " 'explaining': 33,\n",
              " 'exploring': 34,\n",
              " 'for': 35,\n",
              " 'from': 36,\n",
              " 'future': 37,\n",
              " 'guides': 38,\n",
              " 'helps': 39,\n",
              " 'how': 40,\n",
              " 'humans': 41,\n",
              " 'ideas': 42,\n",
              " 'improve': 43,\n",
              " 'innovation': 44,\n",
              " 'inquiry': 45,\n",
              " 'investigating': 46,\n",
              " 'is': 47,\n",
              " 'knowledge': 48,\n",
              " 'life': 49,\n",
              " 'logical': 50,\n",
              " 'method': 51,\n",
              " 'natural': 52,\n",
              " 'new': 53,\n",
              " 'observation': 54,\n",
              " 'of': 55,\n",
              " 'old': 56,\n",
              " 'or': 57,\n",
              " 'our': 58,\n",
              " 'particles': 59,\n",
              " 'patterns': 60,\n",
              " 'phenomena': 61,\n",
              " 'progress': 62,\n",
              " 'provides': 63,\n",
              " 'pursuit': 64,\n",
              " 'reasoning': 65,\n",
              " 'reliable': 66,\n",
              " 'science': 67,\n",
              " 'scientific': 68,\n",
              " 'shapes': 69,\n",
              " 'smallest': 70,\n",
              " 'society': 71,\n",
              " 'studying': 72,\n",
              " 'systematic': 73,\n",
              " 'systems': 74,\n",
              " 'technologies': 75,\n",
              " 'that': 76,\n",
              " 'the': 77,\n",
              " 'theories': 78,\n",
              " 'things': 79,\n",
              " 'thinking': 80,\n",
              " 'through': 81,\n",
              " 'to': 82,\n",
              " 'uncover': 83,\n",
              " 'understand': 84,\n",
              " 'understanding': 85,\n",
              " 'universe': 86,\n",
              " 'us': 87,\n",
              " 'vastness': 88,\n",
              " 'work': 89,\n",
              " 'world': 90}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_words = sorted(list(set(words)))\n",
        "vocab={\n",
        "    '[CLS]':0,\n",
        "    '[SEP]':1,\n",
        "    '[MASK]':2,\n",
        "    '[PAD]':3\n",
        "}\n",
        "for i, word in enumerate(unique_words):\n",
        "  vocab[word]=i+4\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "O6YliBm5VPzE"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    return [self.str_to_int[token] for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip() != '']\n",
        "\n",
        "  def decode(self,token_ids):\n",
        "    return [self.int_to_str[id] for id in token_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SE68ey8bw_Y",
        "outputId": "6d864ec8-1010-4e9d-826e-1660fba50eba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[8, 47, 77, 73]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer=Tokenizer(vocab)\n",
        "\n",
        "ids=tokenizer.encode('Science is the systematic')\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WewQoKAcjMl",
        "outputId": "7347a682-9bec-4b92-ac1c-fe19cf738628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Science', 'is', 'the', 'systematic']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq3CFRjJgjIn"
      },
      "outputs": [],
      "source": [
        "class BertDataset:\n",
        "    def __init__(self, sentence_1, sentence_2, max_len=20, max_pred=10):\n",
        "        self.tokenizer = Tokenizer(vocab)\n",
        "\n",
        "        CLS = vocab['[CLS]']\n",
        "        SEP = vocab['[SEP]']\n",
        "        MASK = vocab['[MASK]']\n",
        "\n",
        "        tokens1 = self.tokenizer.encode(sentence_1)\n",
        "        tokens2 = self.tokenizer.encode(sentence_2)\n",
        "\n",
        "        # build input\n",
        "        input_ids = [CLS] + tokens1 + [SEP] + tokens2 + [SEP]\n",
        "\n",
        "        # segment ids\n",
        "        segment_ids = (\n",
        "            [0] * (1 + len(tokens1) + 1) +\n",
        "            [1] * (len(tokens2) + 1)\n",
        "        )\n",
        "\n",
        "        # ----- MLM MASKING -----\n",
        "        cand_pos = [i for i, tid in enumerate(input_ids) if tid not in [CLS, SEP]]\n",
        "        random.shuffle(cand_pos)\n",
        "\n",
        "        mask_len = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
        "\n",
        "        masked_tokens = []\n",
        "        masked_position = []\n",
        "\n",
        "        for i in range(mask_len):\n",
        "            pos = cand_pos[i]\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            masked_position.append(pos)\n",
        "\n",
        "            prob = random.random()\n",
        "\n",
        "            if prob < 0.8:\n",
        "                input_ids[pos] = MASK  # 80% mask token\n",
        "            elif prob < 0.9:\n",
        "                rand_id = random.randint(0, len(vocab) - 1)\n",
        "                input_ids[pos] = rand_id  # 10% random\n",
        "            else:\n",
        "                pass  # 10% keep original\n",
        "\n",
        "        # pad input to max_len\n",
        "        padding = max_len - len(input_ids)\n",
        "        input_ids += [0] * padding\n",
        "        segment_ids += [0] * padding\n",
        "\n",
        "        # pad masked labels\n",
        "        pad_mlm = max_pred - len(masked_tokens)\n",
        "        masked_tokens += [0] * pad_mlm\n",
        "        masked_position += [0] * pad_mlm\n",
        "\n",
        "        # save\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_tokens = masked_tokens\n",
        "        self.masked_position = masked_position\n",
        "        self.is_next = True  # or False for NSP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NI-3NUbi_ba",
        "outputId": "95c999f5-e3c9-44f3-b94d-8e8d96ed0d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs:  [0, 8, 47, 2, 73, 1, 48, 81, 54, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segment IDs:  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Mask Token:  [77, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Mask Token Position:  [3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "d=BertDataset('Science is the systematic','knowledge through observation')\n",
        "print('Input IDs: ',d.input_ids)\n",
        "print('Segment IDs: ',d.segment_ids)\n",
        "print('Mask Token: ',d.masked_tokens)\n",
        "print('Mask Token Position: ',d.masked_position)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "X5PuvR0Kltyq"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.d_out=d_out\n",
        "    self.d_in=d_in\n",
        "\n",
        "\n",
        "    self.w_k=nn.Linear(d_in,d_out)\n",
        "    self.w_q=nn.Linear(d_in,d_out)\n",
        "    self.w_v=nn.Linear(d_in,d_out)\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys=self.w_k(x)\n",
        "    values=self.w_v(x)\n",
        "    queries=self.w_q(x)\n",
        "\n",
        "    attn_scores=queries @ keys.transpose(-1, -2)\n",
        "    scaled_weight=torch.softmax(attn_scores/torch.sqrt(self.d_out),dim=-1)\n",
        "    attn_matrix=scaled_weight @ values\n",
        "    return attn_matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "cm58HLNZqsVh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.heads=nn.Modulelist(\n",
        "        [SelfAttention(d_in,d_out) for _ in range(num_heads)]\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return torch.cat([head(x) for head in self.heads],dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "s9BzfkLft-_v"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+0.044715*x**3)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "t32fJlFEshTC"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(emb_dim,4*emb_dim),\n",
        "        GELU(),\n",
        "        nn.Linear(4*emb_dim,emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "m1tQ6l-ExpPy"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.eps=1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=torch.mean(x,dim=-1,keepdim=True)\n",
        "    variance=torch.var(x,dim=-1,keepdim=True, unbiased=False)\n",
        "    norm=(x-mean)/torch.sqrt(variance+self.eps)\n",
        "    return self.scale*norm+self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8vaB5cLz6hui"
      },
      "outputs": [],
      "source": [
        "class BERTEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size, emb_dim):\n",
        "    super().__init__()\n",
        "    self.token_embedding=nn.Embedding(vocab_size,emb_dim,segment_token_type=2,max_token=512)\n",
        "    self.segmentation_embedding=nn.Embedding(segment_token_type,emb_dim)\n",
        "    self.position_embedding=nn.Embedding(max_token,emb_dim)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
