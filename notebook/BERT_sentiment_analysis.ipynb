{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "431e087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09f180d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "  def encode(self,text):\n",
    "    return [self.str_to_int[token] for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip() != '']\n",
    "\n",
    "  def decode(self,token_ids):\n",
    "    return [self.int_to_str[id] for id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da04649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self,d_in,d_out):\n",
    "    super().__init__()\n",
    "    self.d_out=d_out\n",
    "    self.d_in=d_in\n",
    "\n",
    "\n",
    "    self.w_k=nn.Linear(d_in,d_out)\n",
    "    self.w_q=nn.Linear(d_in,d_out)\n",
    "    self.w_v=nn.Linear(d_in,d_out)\n",
    "\n",
    "  def forward(self,x):\n",
    "    keys=self.w_k(x)\n",
    "    values=self.w_v(x)\n",
    "    queries=self.w_q(x)\n",
    "\n",
    "    attn_scores=queries @ keys.transpose(-1, -2)\n",
    "    scaled_weight=torch.softmax(attn_scores/torch.sqrt(torch.tensor(self.d_out)),dim=-1)\n",
    "    attn_matrix=scaled_weight @ values\n",
    "    return attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51762326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,num_heads,d_in,d_out):\n",
    "    super().__init__()\n",
    "    self.heads=nn.ModuleList(\n",
    "        [SelfAttention(d_in,d_out) for _ in range(num_heads)]\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return torch.cat([head(x) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daed1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+0.044715*x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb127656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,emb_dim):\n",
    "    super().__init__()\n",
    "    self.layers=nn.Sequential(\n",
    "        nn.Linear(emb_dim,4*emb_dim),\n",
    "        GELU(),\n",
    "        nn.Linear(4*emb_dim,emb_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66a95620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "  def __init__(self,emb_dim):\n",
    "    super().__init__()\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    self.eps=1e-5\n",
    "\n",
    "  def forward(self,x):\n",
    "    mean=torch.mean(x,dim=-1,keepdim=True)\n",
    "    variance=torch.var(x,dim=-1,keepdim=True, unbiased=False)\n",
    "    norm=(x-mean)/torch.sqrt(variance+self.eps)\n",
    "    return self.scale*norm+self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eebffe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, segment_token_type=2, max_token=512, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.segment_embedding = nn.Embedding(segment_token_type, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(max_token, emb_dim)\n",
    "        self.layer_norm = LayerNormalization(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.register_buffer('position_ids', torch.arange(max_token).unsqueeze(0))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, LayerNormalization):\n",
    "            nn.init.ones_(module.scale)\n",
    "            nn.init.zeros_(module.shift)\n",
    "\n",
    "    def forward(self, x, segment_ids=None, position_ids=None):\n",
    "        batch_size, seq_length = x.shape\n",
    "        token_embeds = self.token_embedding(x)\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "\n",
    "        if segment_ids is None:\n",
    "            segment_ids = torch.zeros_like(x, dtype=torch.long)\n",
    "        segment_embeds = self.segment_embedding(segment_ids)\n",
    "\n",
    "        embeddings = token_embeds + segment_embeds + position_embeds\n",
    "\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04f6251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, emb_dim, emb_dim // num_heads)\n",
    "        self.layer_normalization = LayerNormalization(emb_dim)\n",
    "        self.feed_forward = FeedForward(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        attention_weight = self.multihead_attention(x)\n",
    "        x = self.layer_normalization(residual + self.dropout(attention_weight))\n",
    "        residual = x\n",
    "        ff_output = self.feed_forward(x)\n",
    "        output = self.layer_normalization(residual + self.dropout(ff_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d61129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSentimentDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "    super().__init__()\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = self.texts[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    tokens = self.tokenizer.encode(text)\n",
    "\n",
    "    if len(tokens) > self.max_length - 2:\n",
    "        tokens = tokens[:self.max_length - 2]\n",
    "\n",
    "    input_ids = (\n",
    "        [self.tokenizer.str_to_int['[CLS]']] +\n",
    "        tokens +\n",
    "        [self.tokenizer.str_to_int['[SEP]']]\n",
    "    )\n",
    "\n",
    "    padding_length = self.max_length - len(input_ids)\n",
    "    input_ids = input_ids + [self.tokenizer.str_to_int['[PAD]']] * padding_length\n",
    "    segment_ids = [0] * self.max_length\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'segment_ids': torch.tensor(segment_ids, dtype=torch.long),\n",
    "        'label': torch.tensor(label, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4cae7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_custom_data(texts, vocab_size=10000):\n",
    "    token_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [token for token in tokens if token.strip() != '']\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "    print(f\"Total unique tokens found: {len(token_counter)}\")\n",
    "\n",
    "    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "    vocab = {}\n",
    "\n",
    "    for idx, token in enumerate(special_tokens):\n",
    "        vocab[token] = idx\n",
    "\n",
    "    for idx, (token, count) in enumerate(token_counter.most_common(vocab_size - len(special_tokens))):\n",
    "        vocab[token] = idx + len(special_tokens)\n",
    "\n",
    "    print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "    print(vocab)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49eaa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "  def __init__(self, vocab_size, num_classes, emb_dim, num_heads=4, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.embedding = BERTEmbedding(vocab_size, emb_dim, dropout_prob=dropout_prob)\n",
    "    self.encoder = BERTEncoder(emb_dim, num_heads, dropout_prob=dropout_prob)\n",
    "    self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "  def forward(self, input_ids, segment_ids=None):\n",
    "    embeddings = self.embedding(input_ids, segment_ids)\n",
    "    encoder_output = self.encoder(embeddings)\n",
    "    cls_output = encoder_output[:,0,:]\n",
    "    logits = self.classifier(cls_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef59c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    # Positive reviews\n",
    "    (\"This product is amazing! I love it.\", 1),\n",
    "    (\"Excellent quality and fast delivery.\", 1),\n",
    "    (\"Highly recommended, great value for money.\", 1),\n",
    "    (\"Works perfectly, very satisfied with purchase.\", 1),\n",
    "    (\"Outstanding service and product quality.\", 1),\n",
    "    (\"Best purchase I've made this year!\", 1),\n",
    "    (\"Fantastic product, exceeded my expectations.\", 1),\n",
    "    (\"Great features and easy to use.\", 1),\n",
    "    (\"Very happy with this product.\", 1),\n",
    "    (\"Perfect! Exactly what I needed.\", 1),\n",
    "\n",
    "    # Negative reviews\n",
    "    (\"Terrible product, complete waste of money.\", 0),\n",
    "    (\"Poor quality and stopped working after 2 days.\", 0),\n",
    "    (\"Very disappointed with this purchase.\", 0),\n",
    "    (\"Doesn't work as described, avoid this product.\", 0),\n",
    "    (\"Worst product I've ever bought.\", 0),\n",
    "    (\"Broken upon arrival, terrible quality.\", 0),\n",
    "    (\"Not worth the money, very poor performance.\", 0),\n",
    "    (\"Complete garbage, don't buy this.\", 0),\n",
    "    (\"Extremely disappointed, faulty product.\", 0),\n",
    "    (\"Awful experience, product doesn't work.\", 0),\n",
    "\n",
    "    # Neutral reviews\n",
    "    (\"The product is okay, nothing special.\", 2),\n",
    "    (\"It works but could be better.\", 2),\n",
    "    (\"Average product, does the job.\", 2),\n",
    "    (\"Not bad, but not great either.\", 2),\n",
    "    (\"Mediocre quality, expected more.\", 2),\n",
    "]\n",
    "\n",
    "# Separate texts and labels\n",
    "texts = [item[0] for item in sample_data]\n",
    "labels = [item[1] for item in sample_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a02e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens found: 107\n",
      "Final vocabulary size: 112\n",
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '.': 5, ',': 6, 'product': 7, 'quality': 8, \"'\": 9, 'this': 10, 'I': 11, 'and': 12, '!': 13, 'money': 14, 'with': 15, 'purchase': 16, 't': 17, 'is': 18, 'great': 19, 'very': 20, 've': 21, 'Very': 22, 'disappointed': 23, 'work': 24, 'Not': 25, 'the': 26, 'but': 27, 'This': 28, 'amazing': 29, 'love': 30, 'it': 31, 'Excellent': 32, 'fast': 33, 'delivery': 34, 'Highly': 35, 'recommended': 36, 'value': 37, 'for': 38, 'Works': 39, 'perfectly': 40, 'satisfied': 41, 'Outstanding': 42, 'service': 43, 'Best': 44, 'made': 45, 'year': 46, 'Fantastic': 47, 'exceeded': 48, 'my': 49, 'expectations': 50, 'Great': 51, 'features': 52, 'easy': 53, 'to': 54, 'use': 55, 'happy': 56, 'Perfect': 57, 'Exactly': 58, 'what': 59, 'needed': 60, 'Terrible': 61, 'complete': 62, 'waste': 63, 'of': 64, 'Poor': 65, 'stopped': 66, 'working': 67, 'after': 68, '2': 69, 'days': 70, 'Doesn': 71, 'as': 72, 'described': 73, 'avoid': 74, 'Worst': 75, 'ever': 76, 'bought': 77, 'Broken': 78, 'upon': 79, 'arrival': 80, 'terrible': 81, 'worth': 82, 'poor': 83, 'performance': 84, 'Complete': 85, 'garbage': 86, 'don': 87, 'buy': 88, 'Extremely': 89, 'faulty': 90, 'Awful': 91, 'experience': 92, 'doesn': 93, 'The': 94, 'okay': 95, 'nothing': 96, 'special': 97, 'It': 98, 'works': 99, 'could': 100, 'be': 101, 'better': 102, 'Average': 103, 'does': 104, 'job': 105, 'bad': 106, 'not': 107, 'either': 108, 'Mediocre': 109, 'expected': 110, 'more': 111}\n"
     ]
    }
   ],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=100\n",
    "num_classes = len(set(labels))\n",
    "vocab = build_vocab_from_custom_data(texts, vocab_size=5000)\n",
    "tokenizer = Tokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "873559c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            segment_ids = batch['segment_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return 100 * correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54eaa81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 262,275\n",
      "Epoch 1, Batch 0, Loss: 1.1442, Acc: 25.0%\n",
      "\n",
      "Epoch [1/100] completed:\n",
      "  Training Loss: 1.2051\n",
      "  Training Accuracy: 20.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "New best model saved! Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 2, Batch 0, Loss: 1.1906, Acc: 37.5%\n",
      "\n",
      "Epoch [2/100] completed:\n",
      "  Training Loss: 1.1359\n",
      "  Training Accuracy: 30.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 3, Batch 0, Loss: 1.0590, Acc: 50.0%\n",
      "\n",
      "Epoch [3/100] completed:\n",
      "  Training Loss: 1.0244\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 4, Batch 0, Loss: 1.0638, Acc: 62.5%\n",
      "\n",
      "Epoch [4/100] completed:\n",
      "  Training Loss: 1.0311\n",
      "  Training Accuracy: 65.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 5, Batch 0, Loss: 0.9419, Acc: 62.5%\n",
      "\n",
      "Epoch [5/100] completed:\n",
      "  Training Loss: 1.0845\n",
      "  Training Accuracy: 40.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 6, Batch 0, Loss: 1.1430, Acc: 25.0%\n",
      "\n",
      "Epoch [6/100] completed:\n",
      "  Training Loss: 1.0468\n",
      "  Training Accuracy: 40.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 7, Batch 0, Loss: 1.1289, Acc: 37.5%\n",
      "\n",
      "Epoch [7/100] completed:\n",
      "  Training Loss: 0.9841\n",
      "  Training Accuracy: 55.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 8, Batch 0, Loss: 1.0582, Acc: 25.0%\n",
      "\n",
      "Epoch [8/100] completed:\n",
      "  Training Loss: 1.0345\n",
      "  Training Accuracy: 40.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 9, Batch 0, Loss: 0.9545, Acc: 75.0%\n",
      "\n",
      "Epoch [9/100] completed:\n",
      "  Training Loss: 1.0126\n",
      "  Training Accuracy: 55.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 10, Batch 0, Loss: 0.9717, Acc: 62.5%\n",
      "\n",
      "Epoch [10/100] completed:\n",
      "  Training Loss: 1.0698\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 11, Batch 0, Loss: 1.1249, Acc: 25.0%\n",
      "\n",
      "Epoch [11/100] completed:\n",
      "  Training Loss: 1.0150\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 12, Batch 0, Loss: 0.8425, Acc: 87.5%\n",
      "\n",
      "Epoch [12/100] completed:\n",
      "  Training Loss: 1.1310\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 13, Batch 0, Loss: 0.9424, Acc: 62.5%\n",
      "\n",
      "Epoch [13/100] completed:\n",
      "  Training Loss: 1.0489\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 14, Batch 0, Loss: 1.1453, Acc: 25.0%\n",
      "\n",
      "Epoch [14/100] completed:\n",
      "  Training Loss: 1.0565\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 15, Batch 0, Loss: 1.1862, Acc: 12.5%\n",
      "\n",
      "Epoch [15/100] completed:\n",
      "  Training Loss: 1.0640\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 16, Batch 0, Loss: 1.2244, Acc: 50.0%\n",
      "\n",
      "Epoch [16/100] completed:\n",
      "  Training Loss: 1.1189\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 17, Batch 0, Loss: 1.0070, Acc: 62.5%\n",
      "\n",
      "Epoch [17/100] completed:\n",
      "  Training Loss: 1.0559\n",
      "  Training Accuracy: 50.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 18, Batch 0, Loss: 1.0814, Acc: 62.5%\n",
      "\n",
      "Epoch [18/100] completed:\n",
      "  Training Loss: 1.0703\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 19, Batch 0, Loss: 0.9335, Acc: 75.0%\n",
      "\n",
      "Epoch [19/100] completed:\n",
      "  Training Loss: 0.9920\n",
      "  Training Accuracy: 55.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 20, Batch 0, Loss: 1.0020, Acc: 37.5%\n",
      "\n",
      "Epoch [20/100] completed:\n",
      "  Training Loss: 1.0598\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 21, Batch 0, Loss: 1.1264, Acc: 50.0%\n",
      "\n",
      "Epoch [21/100] completed:\n",
      "  Training Loss: 1.1003\n",
      "  Training Accuracy: 55.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 22, Batch 0, Loss: 1.1006, Acc: 37.5%\n",
      "\n",
      "Epoch [22/100] completed:\n",
      "  Training Loss: 1.0454\n",
      "  Training Accuracy: 40.00%\n",
      "  Validation Accuracy: 80.00%\n",
      "New best model saved! Validation Accuracy: 80.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 23, Batch 0, Loss: 1.0212, Acc: 50.0%\n",
      "\n",
      "Epoch [23/100] completed:\n",
      "  Training Loss: 1.0241\n",
      "  Training Accuracy: 40.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 24, Batch 0, Loss: 1.0498, Acc: 37.5%\n",
      "\n",
      "Epoch [24/100] completed:\n",
      "  Training Loss: 1.0202\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 25, Batch 0, Loss: 0.9423, Acc: 62.5%\n",
      "\n",
      "Epoch [25/100] completed:\n",
      "  Training Loss: 1.0875\n",
      "  Training Accuracy: 50.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 26, Batch 0, Loss: 1.1564, Acc: 12.5%\n",
      "\n",
      "Epoch [26/100] completed:\n",
      "  Training Loss: 1.0378\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 27, Batch 0, Loss: 0.9333, Acc: 50.0%\n",
      "\n",
      "Epoch [27/100] completed:\n",
      "  Training Loss: 0.9611\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 28, Batch 0, Loss: 1.0151, Acc: 25.0%\n",
      "\n",
      "Epoch [28/100] completed:\n",
      "  Training Loss: 1.0711\n",
      "  Training Accuracy: 35.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 29, Batch 0, Loss: 0.8981, Acc: 62.5%\n",
      "\n",
      "Epoch [29/100] completed:\n",
      "  Training Loss: 1.0542\n",
      "  Training Accuracy: 45.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 30, Batch 0, Loss: 0.8934, Acc: 75.0%\n",
      "\n",
      "Epoch [30/100] completed:\n",
      "  Training Loss: 0.9636\n",
      "  Training Accuracy: 55.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 31, Batch 0, Loss: 0.9032, Acc: 62.5%\n",
      "\n",
      "Epoch [31/100] completed:\n",
      "  Training Loss: 0.9696\n",
      "  Training Accuracy: 50.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 32, Batch 0, Loss: 0.8624, Acc: 62.5%\n",
      "\n",
      "Epoch [32/100] completed:\n",
      "  Training Loss: 0.9282\n",
      "  Training Accuracy: 60.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 33, Batch 0, Loss: 1.0557, Acc: 50.0%\n",
      "\n",
      "Epoch [33/100] completed:\n",
      "  Training Loss: 0.9764\n",
      "  Training Accuracy: 60.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 34, Batch 0, Loss: 0.9947, Acc: 50.0%\n",
      "\n",
      "Epoch [34/100] completed:\n",
      "  Training Loss: 0.8206\n",
      "  Training Accuracy: 70.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 35, Batch 0, Loss: 0.8102, Acc: 62.5%\n",
      "\n",
      "Epoch [35/100] completed:\n",
      "  Training Loss: 0.7766\n",
      "  Training Accuracy: 70.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 36, Batch 0, Loss: 0.8384, Acc: 75.0%\n",
      "\n",
      "Epoch [36/100] completed:\n",
      "  Training Loss: 0.7198\n",
      "  Training Accuracy: 75.00%\n",
      "  Validation Accuracy: 80.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 37, Batch 0, Loss: 0.5998, Acc: 87.5%\n",
      "\n",
      "Epoch [37/100] completed:\n",
      "  Training Loss: 0.7029\n",
      "  Training Accuracy: 85.00%\n",
      "  Validation Accuracy: 80.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 38, Batch 0, Loss: 0.6402, Acc: 87.5%\n",
      "\n",
      "Epoch [38/100] completed:\n",
      "  Training Loss: 0.6333\n",
      "  Training Accuracy: 90.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 39, Batch 0, Loss: 0.6379, Acc: 87.5%\n",
      "\n",
      "Epoch [39/100] completed:\n",
      "  Training Loss: 0.6583\n",
      "  Training Accuracy: 80.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 40, Batch 0, Loss: 0.5010, Acc: 87.5%\n",
      "\n",
      "Epoch [40/100] completed:\n",
      "  Training Loss: 0.5525\n",
      "  Training Accuracy: 90.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 41, Batch 0, Loss: 0.5367, Acc: 100.0%\n",
      "\n",
      "Epoch [41/100] completed:\n",
      "  Training Loss: 0.4612\n",
      "  Training Accuracy: 95.00%\n",
      "  Validation Accuracy: 80.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 42, Batch 0, Loss: 0.4497, Acc: 100.0%\n",
      "\n",
      "Epoch [42/100] completed:\n",
      "  Training Loss: 0.3967\n",
      "  Training Accuracy: 95.00%\n",
      "  Validation Accuracy: 80.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 43, Batch 0, Loss: 0.3847, Acc: 87.5%\n",
      "\n",
      "Epoch [43/100] completed:\n",
      "  Training Loss: 0.3749\n",
      "  Training Accuracy: 95.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 44, Batch 0, Loss: 0.4230, Acc: 100.0%\n",
      "\n",
      "Epoch [44/100] completed:\n",
      "  Training Loss: 0.3347\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 45, Batch 0, Loss: 0.2005, Acc: 100.0%\n",
      "\n",
      "Epoch [45/100] completed:\n",
      "  Training Loss: 0.3121\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 46, Batch 0, Loss: 0.2181, Acc: 100.0%\n",
      "\n",
      "Epoch [46/100] completed:\n",
      "  Training Loss: 0.2695\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 47, Batch 0, Loss: 0.2040, Acc: 100.0%\n",
      "\n",
      "Epoch [47/100] completed:\n",
      "  Training Loss: 0.2043\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 48, Batch 0, Loss: 0.1467, Acc: 100.0%\n",
      "\n",
      "Epoch [48/100] completed:\n",
      "  Training Loss: 0.1894\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 49, Batch 0, Loss: 0.1241, Acc: 100.0%\n",
      "\n",
      "Epoch [49/100] completed:\n",
      "  Training Loss: 0.1255\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 50, Batch 0, Loss: 0.1047, Acc: 100.0%\n",
      "\n",
      "Epoch [50/100] completed:\n",
      "  Training Loss: 0.1269\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 51, Batch 0, Loss: 0.0961, Acc: 100.0%\n",
      "\n",
      "Epoch [51/100] completed:\n",
      "  Training Loss: 0.1122\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 52, Batch 0, Loss: 0.0860, Acc: 100.0%\n",
      "\n",
      "Epoch [52/100] completed:\n",
      "  Training Loss: 0.0883\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 53, Batch 0, Loss: 0.0755, Acc: 100.0%\n",
      "\n",
      "Epoch [53/100] completed:\n",
      "  Training Loss: 0.0582\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 54, Batch 0, Loss: 0.0524, Acc: 100.0%\n",
      "\n",
      "Epoch [54/100] completed:\n",
      "  Training Loss: 0.0656\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 55, Batch 0, Loss: 0.0284, Acc: 100.0%\n",
      "\n",
      "Epoch [55/100] completed:\n",
      "  Training Loss: 0.0372\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 56, Batch 0, Loss: 0.0520, Acc: 100.0%\n",
      "\n",
      "Epoch [56/100] completed:\n",
      "  Training Loss: 0.0365\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 57, Batch 0, Loss: 0.0176, Acc: 100.0%\n",
      "\n",
      "Epoch [57/100] completed:\n",
      "  Training Loss: 0.0303\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 60.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 58, Batch 0, Loss: 0.0304, Acc: 100.0%\n",
      "\n",
      "Epoch [58/100] completed:\n",
      "  Training Loss: 0.0227\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 59, Batch 0, Loss: 0.0286, Acc: 100.0%\n",
      "\n",
      "Epoch [59/100] completed:\n",
      "  Training Loss: 0.0198\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 60, Batch 0, Loss: 0.0140, Acc: 100.0%\n",
      "\n",
      "Epoch [60/100] completed:\n",
      "  Training Loss: 0.0129\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 61, Batch 0, Loss: 0.0116, Acc: 100.0%\n",
      "\n",
      "Epoch [61/100] completed:\n",
      "  Training Loss: 0.0108\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 62, Batch 0, Loss: 0.0099, Acc: 100.0%\n",
      "\n",
      "Epoch [62/100] completed:\n",
      "  Training Loss: 0.0100\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 63, Batch 0, Loss: 0.0080, Acc: 100.0%\n",
      "\n",
      "Epoch [63/100] completed:\n",
      "  Training Loss: 0.0085\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 64, Batch 0, Loss: 0.0075, Acc: 100.0%\n",
      "\n",
      "Epoch [64/100] completed:\n",
      "  Training Loss: 0.0073\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 65, Batch 0, Loss: 0.0050, Acc: 100.0%\n",
      "\n",
      "Epoch [65/100] completed:\n",
      "  Training Loss: 0.0085\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 66, Batch 0, Loss: 0.0072, Acc: 100.0%\n",
      "\n",
      "Epoch [66/100] completed:\n",
      "  Training Loss: 0.0066\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 67, Batch 0, Loss: 0.0068, Acc: 100.0%\n",
      "\n",
      "Epoch [67/100] completed:\n",
      "  Training Loss: 0.0057\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 68, Batch 0, Loss: 0.0047, Acc: 100.0%\n",
      "\n",
      "Epoch [68/100] completed:\n",
      "  Training Loss: 0.0048\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 69, Batch 0, Loss: 0.0056, Acc: 100.0%\n",
      "\n",
      "Epoch [69/100] completed:\n",
      "  Training Loss: 0.0049\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 70, Batch 0, Loss: 0.0036, Acc: 100.0%\n",
      "\n",
      "Epoch [70/100] completed:\n",
      "  Training Loss: 0.0050\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 71, Batch 0, Loss: 0.0050, Acc: 100.0%\n",
      "\n",
      "Epoch [71/100] completed:\n",
      "  Training Loss: 0.0043\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 72, Batch 0, Loss: 0.0048, Acc: 100.0%\n",
      "\n",
      "Epoch [72/100] completed:\n",
      "  Training Loss: 0.0041\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 73, Batch 0, Loss: 0.0035, Acc: 100.0%\n",
      "\n",
      "Epoch [73/100] completed:\n",
      "  Training Loss: 0.0038\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 74, Batch 0, Loss: 0.0048, Acc: 100.0%\n",
      "\n",
      "Epoch [74/100] completed:\n",
      "  Training Loss: 0.0045\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 75, Batch 0, Loss: 0.0038, Acc: 100.0%\n",
      "\n",
      "Epoch [75/100] completed:\n",
      "  Training Loss: 0.0038\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 76, Batch 0, Loss: 0.0037, Acc: 100.0%\n",
      "\n",
      "Epoch [76/100] completed:\n",
      "  Training Loss: 0.0039\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 77, Batch 0, Loss: 0.0038, Acc: 100.0%\n",
      "\n",
      "Epoch [77/100] completed:\n",
      "  Training Loss: 0.0039\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 78, Batch 0, Loss: 0.0027, Acc: 100.0%\n",
      "\n",
      "Epoch [78/100] completed:\n",
      "  Training Loss: 0.0031\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 79, Batch 0, Loss: 0.0044, Acc: 100.0%\n",
      "\n",
      "Epoch [79/100] completed:\n",
      "  Training Loss: 0.0037\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 80, Batch 0, Loss: 0.0035, Acc: 100.0%\n",
      "\n",
      "Epoch [80/100] completed:\n",
      "  Training Loss: 0.0034\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 81, Batch 0, Loss: 0.0038, Acc: 100.0%\n",
      "\n",
      "Epoch [81/100] completed:\n",
      "  Training Loss: 0.0040\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 82, Batch 0, Loss: 0.0052, Acc: 100.0%\n",
      "\n",
      "Epoch [82/100] completed:\n",
      "  Training Loss: 0.0036\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 83, Batch 0, Loss: 0.0029, Acc: 100.0%\n",
      "\n",
      "Epoch [83/100] completed:\n",
      "  Training Loss: 0.0031\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 84, Batch 0, Loss: 0.0040, Acc: 100.0%\n",
      "\n",
      "Epoch [84/100] completed:\n",
      "  Training Loss: 0.0033\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 85, Batch 0, Loss: 0.0040, Acc: 100.0%\n",
      "\n",
      "Epoch [85/100] completed:\n",
      "  Training Loss: 0.0035\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 86, Batch 0, Loss: 0.0024, Acc: 100.0%\n",
      "\n",
      "Epoch [86/100] completed:\n",
      "  Training Loss: 0.0026\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 87, Batch 0, Loss: 0.0027, Acc: 100.0%\n",
      "\n",
      "Epoch [87/100] completed:\n",
      "  Training Loss: 0.0025\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 88, Batch 0, Loss: 0.0030, Acc: 100.0%\n",
      "\n",
      "Epoch [88/100] completed:\n",
      "  Training Loss: 0.0025\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 89, Batch 0, Loss: 0.0036, Acc: 100.0%\n",
      "\n",
      "Epoch [89/100] completed:\n",
      "  Training Loss: 0.0030\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 90, Batch 0, Loss: 0.0026, Acc: 100.0%\n",
      "\n",
      "Epoch [90/100] completed:\n",
      "  Training Loss: 0.0029\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 91, Batch 0, Loss: 0.0033, Acc: 100.0%\n",
      "\n",
      "Epoch [91/100] completed:\n",
      "  Training Loss: 0.0028\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 92, Batch 0, Loss: 0.0030, Acc: 100.0%\n",
      "\n",
      "Epoch [92/100] completed:\n",
      "  Training Loss: 0.0026\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 93, Batch 0, Loss: 0.0029, Acc: 100.0%\n",
      "\n",
      "Epoch [93/100] completed:\n",
      "  Training Loss: 0.0025\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 94, Batch 0, Loss: 0.0026, Acc: 100.0%\n",
      "\n",
      "Epoch [94/100] completed:\n",
      "  Training Loss: 0.0029\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 95, Batch 0, Loss: 0.0035, Acc: 100.0%\n",
      "\n",
      "Epoch [95/100] completed:\n",
      "  Training Loss: 0.0026\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 96, Batch 0, Loss: 0.0023, Acc: 100.0%\n",
      "\n",
      "Epoch [96/100] completed:\n",
      "  Training Loss: 0.0025\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 97, Batch 0, Loss: 0.0046, Acc: 100.0%\n",
      "\n",
      "Epoch [97/100] completed:\n",
      "  Training Loss: 0.0029\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 98, Batch 0, Loss: 0.0023, Acc: 100.0%\n",
      "\n",
      "Epoch [98/100] completed:\n",
      "  Training Loss: 0.0021\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 99, Batch 0, Loss: 0.0025, Acc: 100.0%\n",
      "\n",
      "Epoch [99/100] completed:\n",
      "  Training Loss: 0.0023\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Epoch 100, Batch 0, Loss: 0.0027, Acc: 100.0%\n",
      "\n",
      "Epoch [100/100] completed:\n",
      "  Training Loss: 0.0025\n",
      "  Training Accuracy: 100.00%\n",
      "  Validation Accuracy: 40.00%\n",
      "------------------------------------------------------------\n",
      "Training completed! Best validation accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomSentimentDataset(texts, labels, tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = SentimentModel(\n",
    "    vocab_size=len(vocab),\n",
    "    num_classes=num_classes,\n",
    "    emb_dim=128,\n",
    "    num_heads=4,\n",
    "    dropout_prob=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "model.train()\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "        labels_batch = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "        total += labels_batch.size(0)\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            batch_accuracy = 100 * (predicted == labels_batch).sum().item() / labels_batch.size(0)\n",
    "            print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {batch_accuracy:.1f}%')\n",
    "\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    val_accuracy = evaluate_model(model, val_loader, device)\n",
    "\n",
    "    print(f'\\nEpoch [{epoch+1}/{epochs}] completed:')\n",
    "    print(f'  Training Loss: {avg_loss:.4f}')\n",
    "    print(f'  Training Accuracy: {epoch_accuracy:.2f}%')\n",
    "    print(f'  Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'tokenizer_vocab': tokenizer.str_to_int,\n",
    "            'vocab_size': len(vocab),\n",
    "            'num_classes': num_classes,\n",
    "            'label_mapping': {\n",
    "                0: 'Negative',\n",
    "                1: 'Positive',\n",
    "                2: 'Neutral'\n",
    "            }\n",
    "        }, 'best_custom_model.pth')\n",
    "        print(f'New best model saved! Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    print('-' * 60)\n",
    "\n",
    "print(f'Training completed! Best validation accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "331f316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \"This product is bad.\"\n",
      "Predicted Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_32656\\2800874424.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model= torch.load('best_custom_model.pth')\n"
     ]
    }
   ],
   "source": [
    "best_model= torch.load('best_custom_model.pth')\n",
    "model.load_state_dict(best_model['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "sample_review=\"This product is bad.\"\n",
    "input_ids = tokenizer.encode(sample_review)\n",
    "input_ids = [tokenizer.str_to_int['[CLS]']] + input_ids + [tokenizer.str_to_int['[SEP]']]\n",
    "padding_length = 128 - len(input_ids)\n",
    "input_ids = input_ids + [tokenizer.str_to_int['[PAD]']] * padding_length\n",
    "input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "    sentiment = best_model['label_mapping'][predicted.item()]\n",
    "    print(f'Review: \"{sample_review}\"')\n",
    "    print(f'Predicted Sentiment: {sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
